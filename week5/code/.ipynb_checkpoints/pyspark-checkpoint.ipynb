{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5b3654",
   "metadata": {},
   "source": [
    "### Importing Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5385eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac364fb",
   "metadata": {},
   "source": [
    "### Starting Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f61836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/Naveen/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/01 03:27:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName('test') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fab71",
   "metadata": {},
   "source": [
    "### Downloading Higher Volume Data Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8e89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-01.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479abc5c",
   "metadata": {},
   "source": [
    "### Creating Spark Dataframe and repartitioning into smaller Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad44066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:28:09|2021-01-01 00:31:42|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|      5.26|      923|              22.28|  0.0|0.67|     1.98|                2.75|       null| 0.0|     14.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:45:56|2021-01-01 00:55:19|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|      3.65|     1382|              18.36|  0.0|0.55|     1.63|                 0.0|       null| 0.0|     17.06|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 00:21:15|2021-01-01 00:22:41|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|      3.51|      849|              14.05|  0.0|0.48|     1.25|                2.75|       null|0.94|     12.98|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 00:39:12|2021-01-01 00:42:37|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|      0.74|      179|               7.91|  0.0|0.24|      0.7|                2.75|       null| 0.0|      7.41|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 00:46:11|2021-01-01 00:47:17|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|       9.2|     1228|              27.11|  0.0|0.81|     2.41|                2.75|       null| 0.0|     22.44|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-01 00:04:00|               null|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|     9.725|     2162|              28.11|  0.0|0.84|     2.49|                2.75|       null| 0.0|      28.9|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-01 00:40:06|               null|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|     2.469|      897|              25.03|  0.0|0.75|     2.22|                 0.0|       null| 0.0|     15.01|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 00:10:36|2021-01-01 00:12:28|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|     13.53|     2157|              29.67|  0.0|1.04|     3.08|                 0.0|       null| 0.0|      34.2|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-01 00:21:17|2021-01-01 00:22:25|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|       1.6|      446|               6.89|  0.0|0.21|     0.61|                 0.0|       null| 0.0|      6.26|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-01 00:36:57|2021-01-01 00:38:09|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|       3.2|      800|              11.51|  0.0|0.53|     1.03|                2.75|       null|2.82|     10.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-01 00:53:31|2021-01-01 00:56:21|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|      5.74|     1257|              17.18|  0.0|0.52|     1.52|                2.75|       null| 0.0|     17.61|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2021-01-01 00:22:58|2021-01-01 00:27:01|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|       1.8|      443|               8.18|  0.0|0.25|     0.73|                2.75|       null| 0.0|      6.12|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2021-01-01 00:46:44|2021-01-01 00:47:49|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|       2.9|      616|               13.1|  0.0|0.45|     1.17|                2.75|       null|0.94|      8.77|                  N|                N|                  |               N|             N|\n",
      "|           HV0004|              B02800|                null|2021-01-01 00:12:50|               null|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|      9.66|     1387|              32.95|  0.0| 0.0|     2.34|                2.75|       null| 0.0|      21.1|                  N|                N|                 N|               N|             N|\n",
      "|           HV0004|              B02800|                null|2021-01-01 00:35:32|               null|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|      4.38|     1305|              22.91|  0.0| 0.0|     1.63|                2.75|       null|3.43|     15.82|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:10:22|2021-01-01 00:11:03|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|      2.03|      373|               7.84|  0.0|0.42|      0.7|                2.75|       null|2.82|      6.93|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:25:00|2021-01-01 00:26:31|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|      3.08|      789|               13.2|  0.0| 0.4|     1.17|                2.75|       null| 0.0|     11.54|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:44:56|2021-01-01 00:49:55|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|      1.17|      310|               7.91|  0.0|0.24|      0.7|                 0.0|       null| 0.0|      6.94|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-01 00:05:04|               null|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|    10.852|     1859|              31.18|  0.0|0.94|     2.77|                 0.0|       null| 0.0|     27.61|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02836|              B02836|2021-01-01 00:40:44|2021-01-01 00:53:34|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|      3.52|     1072|              28.67|  0.0|0.86|     2.54|                 0.0|       null| 0.0|     17.64|                  N|                N|                  |               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a215c14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(originating_base_num,StringType,true),StructField(request_datetime,TimestampType,true),StructField(on_scene_datetime,TimestampType,true),StructField(pickup_datetime,TimestampType,true),StructField(dropoff_datetime,TimestampType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(trip_miles,DoubleType,true),StructField(trip_time,LongType,true),StructField(base_passenger_fare,DoubleType,true),StructField(tolls,DoubleType,true),StructField(bcf,DoubleType,true),StructField(sales_tax,DoubleType,true),StructField(congestion_surcharge,DoubleType,true),StructField(airport_fee,DoubleType,true),StructField(tips,DoubleType,true),StructField(driver_pay,DoubleType,true),StructField(shared_request_flag,StringType,true),StructField(shared_match_flag,StringType,true),StructField(access_a_ride_flag,StringType,true),StructField(wav_request_flag,StringType,true),StructField(wav_match_flag,StringType,true)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b8613d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86510/532987830.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fhvhv_parquet/2021/01/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df.repartition(24) \\\n",
    "    .write \\\n",
    "    .parquet('fhvhv_parquet/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8238e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0005|              B02510|                null|2021-01-14 06:03:37|               null|2021-01-14 06:10:27|2021-01-14 06:16:48|          17|          61|     1.098|      381|                7.5|  0.0|0.23|     0.67|                 0.0|       null| 0.0|      5.47|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-14 06:47:15|               null|2021-01-14 06:53:43|2021-01-14 07:06:18|          61|          71|     2.456|      755|              12.48|  0.0|0.37|     1.11|                 0.0|       null| 0.0|      9.05|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-15 14:07:16|2021-01-15 14:10:33|2021-01-15 14:11:07|2021-01-15 14:41:51|         142|         107|      4.47|     1844|              20.03|  0.0| 0.6|     1.78|                2.75|       null| 0.0|     29.04|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B02889|              B02889|2021-01-29 18:15:35|2021-01-29 18:18:43|2021-01-29 18:20:30|2021-01-29 18:32:03|         159|          78|      2.19|      693|               9.15|  0.0|0.27|     0.81|                 0.0|       null| 0.0|      8.22|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-07 09:05:49|               null|2021-01-07 09:12:19|2021-01-07 09:18:37|         119|         235|      1.47|      378|               7.38|  0.0|0.22|     0.65|                 0.0|       null| 0.0|      5.47|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-11 21:48:07|2021-01-11 21:48:18|2021-01-11 21:49:41|2021-01-11 22:00:20|         158|          87|       3.0|      639|              14.58|  0.0|0.44|     1.29|                2.75|       null| 0.0|      8.66|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02866|              B02866|2021-01-09 07:39:16|2021-01-09 07:47:31|2021-01-09 07:49:14|2021-01-09 08:02:16|         118|           6|       6.5|      782|              21.56|  0.0|0.65|     1.91|                 0.0|       null| 0.0|     13.71|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02872|              B02872|2021-01-27 17:53:06|2021-01-27 17:56:34|2021-01-27 17:56:55|2021-01-27 18:11:04|         116|          50|      4.87|      849|              12.95|  0.0|0.41|      1.2|                2.75|       null| 5.0|     19.14|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B02617|              B02617|2021-01-23 15:12:57|2021-01-23 15:15:09|2021-01-23 15:15:15|2021-01-23 15:25:35|          17|         189|      1.99|      620|              10.75|  0.0|0.32|     0.95|                 0.0|       null| 0.0|      7.39|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-15 18:27:47|               null|2021-01-15 18:35:52|2021-01-15 18:42:15|         117|          86|     0.832|      383|               6.99|  0.0|0.21|     0.62|                 0.0|       null| 0.0|      5.47|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-29 16:02:45|2021-01-29 16:04:37|2021-01-29 16:05:00|2021-01-29 16:20:57|         205|         215|      3.79|      957|               14.7|  0.0|0.44|      1.3|                 0.0|       null| 0.0|     15.19|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-23 22:41:08|               null|2021-01-23 22:42:13|2021-01-23 22:48:40|          36|         198|     1.449|      387|               8.31|  0.0|0.17|     0.51|                 0.0|       null| 0.0|      5.47|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-19 10:43:20|               null|2021-01-19 10:47:12|2021-01-19 10:56:40|          87|          33|     2.223|      568|              13.65| 0.46|0.42|     1.25|                2.75|       null| 0.0|      8.41|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02864|              B02864|2021-01-30 23:41:28|2021-01-30 23:46:31|2021-01-30 23:46:53|2021-01-31 00:00:42|          69|         212|      4.95|      829|              11.69|  0.0|0.35|     1.04|                 0.0|       null| 0.0|      12.4|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2021-01-28 21:53:03|2021-01-28 21:55:46|2021-01-28 21:56:06|2021-01-28 21:59:55|          20|          78|      0.66|      229|                5.7|  0.0|0.17|     0.51|                 0.0|       null| 0.0|      5.39|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-07 20:29:23|2021-01-07 20:32:37|2021-01-07 20:32:43|2021-01-07 20:53:50|          82|          75|      8.83|     1267|              21.63| 6.12|0.83|     2.46|                 0.0|       null| 0.0|     20.35|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02879|              B02879|2021-01-26 20:31:36|2021-01-26 20:35:12|2021-01-26 20:35:42|2021-01-26 20:45:22|          47|          59|      1.61|      580|               8.25|  0.0|0.25|     0.73|                 0.0|       null| 0.0|      6.63|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02765|              B02765|2021-01-21 14:06:35|2021-01-21 14:07:11|2021-01-21 14:07:53|2021-01-21 14:21:52|          41|         159|      2.27|      839|               13.0|  0.0|0.39|     1.15|                 0.0|       null| 0.0|      9.53|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-16 16:06:29|               null|2021-01-16 16:13:08|2021-01-16 16:26:51|          18|         136|     2.295|      823|              10.72|  0.0|0.32|     0.95|                 0.0|       null| 0.0|      9.44|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-16 00:12:43|               null|2021-01-16 00:16:23|2021-01-16 00:26:12|         210|          11|     3.745|      589|              11.61|  0.0|0.35|     1.03|                 0.0|       null| 0.0|      9.09|                  N|                N|                 N|               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.parquet('fhvhv_parquet/2021/01/')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32c32f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b6f29",
   "metadata": {},
   "source": [
    "### Changing Schema of Higher Volume Data Unsuccessfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "facfea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "schema = types.StructType([\n",
    "  types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "  types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "  types.StructField('originating_base_num', types.StringType(), True),\n",
    "  types.StructField('request_datetime', types.TimestampType(), True),\n",
    "  types.StructField('on_scene_datetime', types.TimestampType(), True),\n",
    "  types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "  types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "  types.StructField('PULocationID', types.IntegerType(), True),\n",
    "  types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "  types.StructField('trip_miles', types.DoubleType(), True),\n",
    "  types.StructField('trip_time', types.LongType(), True),\n",
    "  types.StructField('base_passenger_fare', types.DoubleType(), True),\n",
    "  types.StructField('tolls', types.DoubleType(), True),\n",
    "  types.StructField('bcf', types.DoubleType(), True),\n",
    "  types.StructField('sales_tax', types.DoubleType(), True),\n",
    "  types.StructField('congestion_surcharge', types.DoubleType(), True),\n",
    "  types.StructField('airport_fee', types.DoubleType(), True),\n",
    "  types.StructField('tips', types.DoubleType(), True),\n",
    "  types.StructField('driver_pay', types.DoubleType(), True),\n",
    "  types.StructField('shared_request_flag', types.StringType(), True),\n",
    "  types.StructField('shared_match_flag', types.StringType(), True),\n",
    "  types.StructField('access_a_ride_flag', types.StringType(), True),\n",
    "  types.StructField('wav_request_flag', types.StringType(), True),\n",
    "  types.StructField('wav_match_flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dec3bbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/01 04:17:55 ERROR Executor: Exception in task 0.0 in stage 13.0 (TID 45)3]\n",
      "java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary\n",
      "\tat org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToInt(ParquetDictionary.java:31)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:298)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/06/01 04:17:55 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 45, ny-trips-vm.europe-west1-b.c.ny-trips-de.internal, executor driver): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary\n",
      "\tat org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToInt(ParquetDictionary.java:31)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:298)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "22/06/01 04:17:55 ERROR TaskSetManager: Task 0 in stage 13.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o74.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 45, ny-trips-vm.europe-west1-b.c.ny-trips-de.internal, executor driver): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToInt(ParquetDictionary.java:31)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:298)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToInt(ParquetDictionary.java:31)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:298)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86510/492303435.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fhvhv_tripdata_2021-01.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o74.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 45, ny-trips-vm.europe-west1-b.c.ny-trips-de.internal, executor driver): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToInt(ParquetDictionary.java:31)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:298)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToInt(ParquetDictionary.java:31)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:298)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.schema(schema).parquet('fhvhv_tripdata_2021-01.parquet')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d113b0",
   "metadata": {},
   "source": [
    "### Repartitioning Yellow and Green Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a7eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t\t   fhvhv_tripdata_2021-01.parquet  taxi+_zone_lookup.csv\r\n",
      "download_data.sh   head.csv\t\t\t   test.ipynb\r\n",
      "fhv_tripdata1.csv  pyspark.ipynb\t\t   zones\r\n",
      "fhvhv_parquet\t   schema.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4509805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2019-12-18 15:52:30|  2019-12-18 15:54:39|                 N|       1.0|         264|         264|            5.0|          0.0|        3.5|  0.5|    0.5|      0.01|         0.0|     null|                  0.3|        4.81|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:45:58|  2020-01-01 00:56:39|                 N|       5.0|          66|          65|            2.0|         1.28|       20.0|  0.0|    0.0|      4.06|         0.0|     null|                  0.3|       24.36|         1.0|      2.0|                 0.0|\n",
      "|       2| 2020-01-01 00:41:38|  2020-01-01 00:52:49|                 N|       1.0|         181|         228|            1.0|         2.47|       10.5|  0.5|    0.5|      3.54|         0.0|     null|                  0.3|       15.34|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:52:46|  2020-01-01 01:14:21|                 N|       1.0|         129|         263|            2.0|          6.3|       21.0| 3.25|    0.5|       0.0|         0.0|     null|                  0.3|       25.05|         2.0|      1.0|                2.75|\n",
      "|       1| 2020-01-01 00:19:57|  2020-01-01 00:30:56|                 N|       1.0|         210|         150|            1.0|          2.3|       10.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        11.3|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:52:33|  2020-01-01 01:09:54|                 N|       1.0|          35|          39|            1.0|          3.0|       13.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        14.8|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:10:18|  2020-01-01 00:22:16|                 N|       1.0|          25|          61|            1.0|         2.77|       11.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        12.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 01:03:14|  2020-01-01 01:29:45|                 N|       1.0|         225|          89|            1.0|         4.98|       20.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        21.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:04:11|  2020-01-01 00:09:48|                 N|       1.0|         129|         129|            1.0|         0.71|        5.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         6.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:25:52|  2020-01-01 00:32:16|                 N|       1.0|         129|          83|            1.0|          0.8|        5.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         6.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:47:32|  2020-01-01 00:59:25|                 N|       1.0|          82|         173|            1.0|         1.52|        9.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        10.8|         2.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:26:40|  2020-01-01 00:40:42|                 N|       1.0|          74|          69|            1.0|          3.8|       14.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        15.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:38:47|  2020-01-01 00:46:02|                 N|       1.0|          74|          41|            1.0|         1.12|        6.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         7.8|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:52:18|  2020-01-01 01:09:58|                 N|       1.0|          41|         127|            1.0|         5.67|       19.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        20.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:16:01|  2020-01-01 00:26:40|                 N|       1.0|           7|         260|            1.0|         1.86|        9.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        10.8|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:21:15|  2020-01-01 00:28:03|                 N|       1.0|           7|           7|            1.0|         1.42|        7.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.3|         2.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:35:42|  2020-01-01 01:02:00|                 N|       1.0|           7|         133|            1.0|        15.48|       43.0|  0.5|    0.5|      8.86|         0.0|     null|                  0.3|       53.16|         1.0|      1.0|                 0.0|\n",
      "|       2| 2020-01-01 00:39:51|  2020-01-01 00:42:54|                 N|       1.0|         134|          28|            1.0|         1.15|        5.5|  0.5|    0.5|       1.0|         0.0|     null|                  0.3|         7.8|         1.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:00:21|  2020-01-01 00:10:19|                 N|       1.0|          89|          39|            1.0|          2.3|       10.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        11.3|         2.0|      1.0|                 0.0|\n",
      "|       1| 2020-01-01 00:13:59|  2020-01-01 00:21:31|                 N|       1.0|          66|          65|            3.0|          1.0|        6.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         7.8|         2.0|      1.0|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green = spark.read \\\n",
    "                .option(\"headers\", \"true\") \\\n",
    "                .parquet(\"data/pq/green/2020/01/\")\n",
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72cf688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aeb437f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2020-01-01 00:28:15|  2020-01-01 00:33:03|            1.0|          1.2|       1.0|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:35:39|  2020-01-01 00:43:04|            1.0|          1.2|       1.0|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:47:41|  2020-01-01 00:53:52|            1.0|          0.6|       1.0|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:55:23|  2020-01-01 01:00:14|            1.0|          0.8|       1.0|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|       null|\n",
      "|       2| 2020-01-01 00:01:58|  2020-01-01 00:04:16|            1.0|          0.0|       1.0|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|       null|\n",
      "|       2| 2020-01-01 00:09:44|  2020-01-01 00:10:37|            1.0|         0.03|       1.0|                 N|           7|         193|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|\n",
      "|       2| 2020-01-01 00:39:25|  2020-01-01 00:39:29|            1.0|          0.0|       1.0|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      0.01|         0.0|                  0.3|        3.81|                 0.0|       null|\n",
      "|       2| 2019-12-18 15:27:49|  2019-12-18 15:28:59|            1.0|          0.0|       5.0|                 N|         193|         193|           1|       0.01|  0.0|    0.0|       0.0|         0.0|                  0.3|        2.81|                 2.5|       null|\n",
      "|       2| 2019-12-18 15:30:35|  2019-12-18 15:31:35|            4.0|          0.0|       1.0|                 N|         193|         193|           1|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:29:01|  2020-01-01 00:40:28|            2.0|          0.7|       1.0|                 N|         246|          48|           1|        8.0|  3.0|    0.5|      2.35|         0.0|                  0.3|       14.15|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:55:11|  2020-01-01 01:12:03|            2.0|          2.4|       1.0|                 N|         246|          79|           1|       12.0|  3.0|    0.5|      1.75|         0.0|                  0.3|       17.55|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:37:15|  2020-01-01 00:51:41|            1.0|          0.8|       1.0|                 N|         163|         161|           2|        9.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:56:27|  2020-01-01 01:21:44|            1.0|          3.3|       1.0|                 N|         161|         144|           1|       17.0|  3.0|    0.5|      4.15|         0.0|                  0.3|       24.95|                 2.5|       null|\n",
      "|       2| 2020-01-01 00:21:54|  2020-01-01 00:27:31|            1.0|         1.07|       1.0|                 N|          43|         239|           1|        6.0|  0.5|    0.5|      1.96|         0.0|                  0.3|       11.76|                 2.5|       null|\n",
      "|       2| 2020-01-01 00:38:01|  2020-01-01 01:15:21|            1.0|         7.76|       1.0|                 N|         143|          25|           1|       28.5|  0.5|    0.5|      4.84|         0.0|                  0.3|       37.14|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:15:35|  2020-01-01 00:27:06|            3.0|          1.6|       1.0|                 N|         211|         234|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:41:20|  2020-01-01 00:44:22|            1.0|          0.5|       1.0|                 Y|         234|          90|           1|        4.0|  3.0|    0.5|       1.0|         0.0|                  0.3|         8.8|                 2.5|       null|\n",
      "|       1| 2020-01-01 00:56:38|  2020-01-01 01:13:34|            1.0|          1.7|       1.0|                 N|         246|         142|           2|       11.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        15.3|                 2.5|       null|\n",
      "|       2| 2020-01-01 00:08:21|  2020-01-01 00:25:29|            1.0|         8.45|       1.0|                 N|         138|         216|           2|       24.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        25.8|                 0.0|       null|\n",
      "|       1| 2020-01-01 00:25:39|  2020-01-01 00:27:05|            1.0|          0.0|       1.0|                 N|         170|         162|           4|        3.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                 2.5|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow = spark.read \\\n",
    "                .option(\"headers\", \"true\") \\\n",
    "                .parquet(\"data/raw/yellow/2020/01/\")\n",
    "df_yellow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e750604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "914d11dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_tripdata_2020_01.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/raw/yellow/2020/01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5934f2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw/green/2020/01\n",
      "data/raw/green/2020/01\n",
      "data/raw/green/2020/02\n",
      "data/raw/green/2020/02\n",
      "data/raw/green/2020/03\n",
      "data/raw/green/2020/03\n",
      "data/raw/green/2020/04\n",
      "data/raw/green/2020/04\n",
      "data/raw/green/2020/05\n",
      "data/raw/green/2020/05\n",
      "data/raw/green/2020/06\n",
      "data/raw/green/2020/06\n",
      "data/raw/green/2020/07\n",
      "data/raw/green/2020/07\n",
      "data/raw/green/2020/08\n",
      "data/raw/green/2020/08\n",
      "data/raw/green/2020/09\n",
      "data/raw/green/2020/09\n",
      "data/raw/green/2020/10\n",
      "data/raw/green/2020/10\n",
      "data/raw/green/2020/11\n",
      "data/raw/green/2020/11\n",
      "data/raw/green/2020/12\n",
      "data/raw/green/2020/12\n",
      "data/raw/green/2021/01\n",
      "data/raw/green/2021/01\n",
      "data/raw/green/2021/02\n",
      "data/raw/green/2021/02\n",
      "data/raw/green/2021/03\n",
      "data/raw/green/2021/03\n",
      "data/raw/green/2021/04\n",
      "data/raw/green/2021/04\n",
      "data/raw/green/2021/05\n",
      "data/raw/green/2021/05\n",
      "data/raw/green/2021/06\n",
      "data/raw/green/2021/06\n",
      "data/raw/green/2021/07\n",
      "data/raw/green/2021/07\n",
      "data/raw/green/2021/08\n",
      "data/raw/green/2021/08\n",
      "data/raw/green/2021/09\n",
      "data/raw/green/2021/09\n",
      "data/raw/green/2021/10\n",
      "data/raw/green/2021/10\n",
      "data/raw/green/2021/11\n",
      "data/raw/green/2021/11\n",
      "data/raw/green/2021/12\n",
      "data/raw/green/2021/12\n",
      "data/raw/yellow/2020/01\n",
      "data/raw/yellow/2020/01\n",
      "data/raw/yellow/2020/02\n",
      "data/raw/yellow/2020/02\n",
      "data/raw/yellow/2020/03\n",
      "data/raw/yellow/2020/03\n",
      "data/raw/yellow/2020/04\n",
      "data/raw/yellow/2020/04\n",
      "data/raw/yellow/2020/05\n",
      "data/raw/yellow/2020/05\n",
      "data/raw/yellow/2020/06\n",
      "data/raw/yellow/2020/06\n",
      "data/raw/yellow/2020/07\n",
      "data/raw/yellow/2020/07\n",
      "data/raw/yellow/2020/08\n",
      "data/raw/yellow/2020/08\n",
      "data/raw/yellow/2020/09\n",
      "data/raw/yellow/2020/09\n",
      "data/raw/yellow/2020/10\n",
      "data/raw/yellow/2020/10\n",
      "data/raw/yellow/2020/11\n",
      "data/raw/yellow/2020/11\n",
      "data/raw/yellow/2020/12\n",
      "data/raw/yellow/2020/12\n",
      "data/raw/yellow/2021/01\n",
      "data/raw/yellow/2021/01\n",
      "data/raw/yellow/2021/02\n",
      "data/raw/yellow/2021/02\n",
      "data/raw/yellow/2021/03\n",
      "data/raw/yellow/2021/03\n",
      "data/raw/yellow/2021/04\n",
      "data/raw/yellow/2021/04\n",
      "data/raw/yellow/2021/05\n",
      "data/raw/yellow/2021/05\n",
      "data/raw/yellow/2021/06\n",
      "data/raw/yellow/2021/06\n",
      "data/raw/yellow/2021/07\n",
      "data/raw/yellow/2021/07\n",
      "data/raw/yellow/2021/08\n",
      "data/raw/yellow/2021/08\n",
      "data/raw/yellow/2021/09\n",
      "data/raw/yellow/2021/09\n",
      "data/raw/yellow/2021/10\n",
      "data/raw/yellow/2021/10\n",
      "data/raw/yellow/2021/11\n",
      "data/raw/yellow/2021/11\n",
      "data/raw/yellow/2021/12\n",
      "data/raw/yellow/2021/12\n"
     ]
    }
   ],
   "source": [
    "years = [2020, 2021]\n",
    "colors = ['green', 'yellow']\n",
    "\n",
    "for color in colors:\n",
    "    for year in years:\n",
    "        for month in range(1,13):            \n",
    "#             print(f'Processing Data for {year}/{month}')\n",
    "\n",
    "            input_path=f'data/raw/{color}/{year}/{month:02d}/'\n",
    "            out_path=f'data/pq/{color}/{year}/{month:02d}/'\n",
    "            print(input_path)\n",
    "            print(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bec38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
